{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage import io\n",
    "from keras.models import Model, load_model\n",
    "from keras.applications import inception_v3\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras import losses, optimizers\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import callbacks\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "RANDOM_SEED = 43\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (2992, 448, 448, 3)\n",
      "y_train.shape: (2992,)\n",
      "X_val.shape: (997, 448, 448, 3)\n",
      "y_val.shape: (997,)\n",
      "X_test.shape: (1000, 448, 448, 3)\n",
      "y_test.shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "X_train, y_train = np.load('./data/X_train.npy'), np.load('./data/y_train.npy')\n",
    "X_val, y_val = np.load('./data/X_val.npy'), np.load('./data/y_val.npy')\n",
    "X_test, y_test = np.load('./data/X_test.npy'), np.load('./data/y_test.npy')\n",
    "print('X_train.shape:', X_train.shape)\n",
    "print('y_train.shape:', y_train.shape)\n",
    "print('X_val.shape:', X_val.shape)\n",
    "print('y_val.shape:', y_val.shape)\n",
    "print('X_test.shape:', X_test.shape)\n",
    "print('y_test.shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess function\n",
    "def preprocess_im(im):\n",
    "    im = im.astype(np.float32)\n",
    "    im = inception_v3.preprocess_input(im)\n",
    "    return im\n",
    "\n",
    "# Preprocess validation\n",
    "X_val = inception_v3.preprocess_input(X_val.astype(np.float32))\n",
    "\n",
    "# Preprocess y\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_val = np_utils.to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.applications.inception_v3' has no attribute 'myInceptionV3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-217b637d1cdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ** Make model **\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Base model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minception_v3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyInceptionV3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m448\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m448\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# base_model = inception_v3.myInceptionV3(weights='imagenet', include_top=False, input_shape=(499, 499, 3))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'keras.applications.inception_v3' has no attribute 'myInceptionV3'"
     ]
    }
   ],
   "source": [
    "# ** Make model **\n",
    "# Base model\n",
    "base_model = inception_v3.myInceptionV3(weights='imagenet', include_top=False, input_shape=(448, 448, 3))\n",
    "# base_model = inception_v3.myInceptionV3(weights='imagenet', include_top=False, input_shape=(499, 499, 3))\n",
    "# Model\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.33)(x)\n",
    "x = Dense(2, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 499, 499, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_750 (Conv2D)             (None, 249, 249, 32) 864         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_750 (BatchN (None, 249, 249, 32) 96          conv2d_750[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_749 (Activation)     (None, 249, 249, 32) 0           batch_normalization_750[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_751 (Conv2D)             (None, 247, 247, 32) 9216        activation_749[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_751 (BatchN (None, 247, 247, 32) 96          conv2d_751[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_750 (Activation)     (None, 247, 247, 32) 0           batch_normalization_751[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_752 (Conv2D)             (None, 247, 247, 64) 18432       activation_750[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_752 (BatchN (None, 247, 247, 64) 192         conv2d_752[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_751 (Activation)     (None, 247, 247, 64) 0           batch_normalization_752[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling2D) (None, 123, 123, 64) 0           activation_751[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_753 (Conv2D)             (None, 123, 123, 80) 5120        max_pooling2d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_753 (BatchN (None, 123, 123, 80) 240         conv2d_753[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_752 (Activation)     (None, 123, 123, 80) 0           batch_normalization_753[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_754 (Conv2D)             (None, 121, 121, 192 138240      activation_752[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_754 (BatchN (None, 121, 121, 192 576         conv2d_754[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_753 (Activation)     (None, 121, 121, 192 0           batch_normalization_754[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling2D) (None, 60, 60, 192)  0           activation_753[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_758 (Conv2D)             (None, 60, 60, 64)   12288       max_pooling2d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_758 (BatchN (None, 60, 60, 64)   192         conv2d_758[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_757 (Activation)     (None, 60, 60, 64)   0           batch_normalization_758[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_756 (Conv2D)             (None, 60, 60, 48)   9216        max_pooling2d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_759 (Conv2D)             (None, 60, 60, 96)   55296       activation_757[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_756 (BatchN (None, 60, 60, 48)   144         conv2d_756[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_759 (BatchN (None, 60, 60, 96)   288         conv2d_759[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_755 (Activation)     (None, 60, 60, 48)   0           batch_normalization_756[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_758 (Activation)     (None, 60, 60, 96)   0           batch_normalization_759[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_72 (AveragePo (None, 60, 60, 192)  0           max_pooling2d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_755 (Conv2D)             (None, 60, 60, 64)   12288       max_pooling2d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_757 (Conv2D)             (None, 60, 60, 64)   76800       activation_755[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_760 (Conv2D)             (None, 60, 60, 96)   82944       activation_758[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_761 (Conv2D)             (None, 60, 60, 32)   6144        average_pooling2d_72[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_755 (BatchN (None, 60, 60, 64)   192         conv2d_755[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_757 (BatchN (None, 60, 60, 64)   192         conv2d_757[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_760 (BatchN (None, 60, 60, 96)   288         conv2d_760[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_761 (BatchN (None, 60, 60, 32)   96          conv2d_761[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_754 (Activation)     (None, 60, 60, 64)   0           batch_normalization_755[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_756 (Activation)     (None, 60, 60, 64)   0           batch_normalization_757[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_759 (Activation)     (None, 60, 60, 96)   0           batch_normalization_760[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_760 (Activation)     (None, 60, 60, 32)   0           batch_normalization_761[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 60, 60, 256)  0           activation_754[0][0]             \n",
      "                                                                 activation_756[0][0]             \n",
      "                                                                 activation_759[0][0]             \n",
      "                                                                 activation_760[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_765 (Conv2D)             (None, 60, 60, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_765 (BatchN (None, 60, 60, 64)   192         conv2d_765[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_764 (Activation)     (None, 60, 60, 64)   0           batch_normalization_765[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_763 (Conv2D)             (None, 60, 60, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_766 (Conv2D)             (None, 60, 60, 96)   55296       activation_764[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_763 (BatchN (None, 60, 60, 48)   144         conv2d_763[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_766 (BatchN (None, 60, 60, 96)   288         conv2d_766[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_762 (Activation)     (None, 60, 60, 48)   0           batch_normalization_763[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_765 (Activation)     (None, 60, 60, 96)   0           batch_normalization_766[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_73 (AveragePo (None, 60, 60, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_762 (Conv2D)             (None, 60, 60, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_764 (Conv2D)             (None, 60, 60, 64)   76800       activation_762[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_767 (Conv2D)             (None, 60, 60, 96)   82944       activation_765[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_768 (Conv2D)             (None, 60, 60, 64)   16384       average_pooling2d_73[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_762 (BatchN (None, 60, 60, 64)   192         conv2d_762[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_764 (BatchN (None, 60, 60, 64)   192         conv2d_764[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_767 (BatchN (None, 60, 60, 96)   288         conv2d_767[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_768 (BatchN (None, 60, 60, 64)   192         conv2d_768[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_761 (Activation)     (None, 60, 60, 64)   0           batch_normalization_762[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_763 (Activation)     (None, 60, 60, 64)   0           batch_normalization_764[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_766 (Activation)     (None, 60, 60, 96)   0           batch_normalization_767[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_767 (Activation)     (None, 60, 60, 64)   0           batch_normalization_768[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 60, 60, 288)  0           activation_761[0][0]             \n",
      "                                                                 activation_763[0][0]             \n",
      "                                                                 activation_766[0][0]             \n",
      "                                                                 activation_767[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_772 (Conv2D)             (None, 60, 60, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_772 (BatchN (None, 60, 60, 64)   192         conv2d_772[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_771 (Activation)     (None, 60, 60, 64)   0           batch_normalization_772[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_770 (Conv2D)             (None, 60, 60, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_773 (Conv2D)             (None, 60, 60, 96)   55296       activation_771[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_770 (BatchN (None, 60, 60, 48)   144         conv2d_770[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_773 (BatchN (None, 60, 60, 96)   288         conv2d_773[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_769 (Activation)     (None, 60, 60, 48)   0           batch_normalization_770[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_772 (Activation)     (None, 60, 60, 96)   0           batch_normalization_773[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_74 (AveragePo (None, 60, 60, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_769 (Conv2D)             (None, 60, 60, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_771 (Conv2D)             (None, 60, 60, 64)   76800       activation_769[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_774 (Conv2D)             (None, 60, 60, 96)   82944       activation_772[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_775 (Conv2D)             (None, 60, 60, 64)   18432       average_pooling2d_74[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_769 (BatchN (None, 60, 60, 64)   192         conv2d_769[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_771 (BatchN (None, 60, 60, 64)   192         conv2d_771[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_774 (BatchN (None, 60, 60, 96)   288         conv2d_774[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_775 (BatchN (None, 60, 60, 64)   192         conv2d_775[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_768 (Activation)     (None, 60, 60, 64)   0           batch_normalization_769[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_770 (Activation)     (None, 60, 60, 64)   0           batch_normalization_771[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_773 (Activation)     (None, 60, 60, 96)   0           batch_normalization_774[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_774 (Activation)     (None, 60, 60, 64)   0           batch_normalization_775[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 60, 60, 288)  0           activation_768[0][0]             \n",
      "                                                                 activation_770[0][0]             \n",
      "                                                                 activation_773[0][0]             \n",
      "                                                                 activation_774[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_777 (Conv2D)             (None, 60, 60, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_777 (BatchN (None, 60, 60, 64)   192         conv2d_777[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_776 (Activation)     (None, 60, 60, 64)   0           batch_normalization_777[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_778 (Conv2D)             (None, 60, 60, 96)   55296       activation_776[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_778 (BatchN (None, 60, 60, 96)   288         conv2d_778[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_777 (Activation)     (None, 60, 60, 96)   0           batch_normalization_778[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_776 (Conv2D)             (None, 29, 29, 384)  995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_779 (Conv2D)             (None, 29, 29, 96)   82944       activation_777[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_776 (BatchN (None, 29, 29, 384)  1152        conv2d_776[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_779 (BatchN (None, 29, 29, 96)   288         conv2d_779[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_775 (Activation)     (None, 29, 29, 384)  0           batch_normalization_776[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_778 (Activation)     (None, 29, 29, 96)   0           batch_normalization_779[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling2D) (None, 29, 29, 288)  0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 29, 29, 768)  0           activation_775[0][0]             \n",
      "                                                                 activation_778[0][0]             \n",
      "                                                                 max_pooling2d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_784 (Conv2D)             (None, 29, 29, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_784 (BatchN (None, 29, 29, 128)  384         conv2d_784[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_783 (Activation)     (None, 29, 29, 128)  0           batch_normalization_784[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_785 (Conv2D)             (None, 29, 29, 128)  114688      activation_783[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_785 (BatchN (None, 29, 29, 128)  384         conv2d_785[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_784 (Activation)     (None, 29, 29, 128)  0           batch_normalization_785[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_781 (Conv2D)             (None, 29, 29, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_786 (Conv2D)             (None, 29, 29, 128)  114688      activation_784[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_781 (BatchN (None, 29, 29, 128)  384         conv2d_781[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_786 (BatchN (None, 29, 29, 128)  384         conv2d_786[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_780 (Activation)     (None, 29, 29, 128)  0           batch_normalization_781[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_785 (Activation)     (None, 29, 29, 128)  0           batch_normalization_786[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_782 (Conv2D)             (None, 29, 29, 128)  114688      activation_780[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_787 (Conv2D)             (None, 29, 29, 128)  114688      activation_785[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_782 (BatchN (None, 29, 29, 128)  384         conv2d_782[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_787 (BatchN (None, 29, 29, 128)  384         conv2d_787[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_781 (Activation)     (None, 29, 29, 128)  0           batch_normalization_782[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_786 (Activation)     (None, 29, 29, 128)  0           batch_normalization_787[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_75 (AveragePo (None, 29, 29, 768)  0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_780 (Conv2D)             (None, 29, 29, 192)  147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_783 (Conv2D)             (None, 29, 29, 192)  172032      activation_781[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_788 (Conv2D)             (None, 29, 29, 192)  172032      activation_786[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_789 (Conv2D)             (None, 29, 29, 192)  147456      average_pooling2d_75[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_780 (BatchN (None, 29, 29, 192)  576         conv2d_780[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_783 (BatchN (None, 29, 29, 192)  576         conv2d_783[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_788 (BatchN (None, 29, 29, 192)  576         conv2d_788[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_789 (BatchN (None, 29, 29, 192)  576         conv2d_789[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_779 (Activation)     (None, 29, 29, 192)  0           batch_normalization_780[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_782 (Activation)     (None, 29, 29, 192)  0           batch_normalization_783[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_787 (Activation)     (None, 29, 29, 192)  0           batch_normalization_788[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_788 (Activation)     (None, 29, 29, 192)  0           batch_normalization_789[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 29, 29, 768)  0           activation_779[0][0]             \n",
      "                                                                 activation_782[0][0]             \n",
      "                                                                 activation_787[0][0]             \n",
      "                                                                 activation_788[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_794 (Conv2D)             (None, 29, 29, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_794 (BatchN (None, 29, 29, 160)  480         conv2d_794[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_793 (Activation)     (None, 29, 29, 160)  0           batch_normalization_794[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_795 (Conv2D)             (None, 29, 29, 160)  179200      activation_793[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_795 (BatchN (None, 29, 29, 160)  480         conv2d_795[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_794 (Activation)     (None, 29, 29, 160)  0           batch_normalization_795[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_791 (Conv2D)             (None, 29, 29, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_796 (Conv2D)             (None, 29, 29, 160)  179200      activation_794[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_791 (BatchN (None, 29, 29, 160)  480         conv2d_791[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_796 (BatchN (None, 29, 29, 160)  480         conv2d_796[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_790 (Activation)     (None, 29, 29, 160)  0           batch_normalization_791[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_795 (Activation)     (None, 29, 29, 160)  0           batch_normalization_796[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_792 (Conv2D)             (None, 29, 29, 160)  179200      activation_790[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_797 (Conv2D)             (None, 29, 29, 160)  179200      activation_795[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_792 (BatchN (None, 29, 29, 160)  480         conv2d_792[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_797 (BatchN (None, 29, 29, 160)  480         conv2d_797[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_791 (Activation)     (None, 29, 29, 160)  0           batch_normalization_792[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_796 (Activation)     (None, 29, 29, 160)  0           batch_normalization_797[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_76 (AveragePo (None, 29, 29, 768)  0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_790 (Conv2D)             (None, 29, 29, 192)  147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_793 (Conv2D)             (None, 29, 29, 192)  215040      activation_791[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_798 (Conv2D)             (None, 29, 29, 192)  215040      activation_796[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_799 (Conv2D)             (None, 29, 29, 192)  147456      average_pooling2d_76[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_790 (BatchN (None, 29, 29, 192)  576         conv2d_790[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_793 (BatchN (None, 29, 29, 192)  576         conv2d_793[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_798 (BatchN (None, 29, 29, 192)  576         conv2d_798[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_799 (BatchN (None, 29, 29, 192)  576         conv2d_799[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_789 (Activation)     (None, 29, 29, 192)  0           batch_normalization_790[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_792 (Activation)     (None, 29, 29, 192)  0           batch_normalization_793[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_797 (Activation)     (None, 29, 29, 192)  0           batch_normalization_798[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_798 (Activation)     (None, 29, 29, 192)  0           batch_normalization_799[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 29, 29, 768)  0           activation_789[0][0]             \n",
      "                                                                 activation_792[0][0]             \n",
      "                                                                 activation_797[0][0]             \n",
      "                                                                 activation_798[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_804 (Conv2D)             (None, 29, 29, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_804 (BatchN (None, 29, 29, 160)  480         conv2d_804[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_803 (Activation)     (None, 29, 29, 160)  0           batch_normalization_804[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_805 (Conv2D)             (None, 29, 29, 160)  179200      activation_803[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_805 (BatchN (None, 29, 29, 160)  480         conv2d_805[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_804 (Activation)     (None, 29, 29, 160)  0           batch_normalization_805[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_801 (Conv2D)             (None, 29, 29, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_806 (Conv2D)             (None, 29, 29, 160)  179200      activation_804[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_801 (BatchN (None, 29, 29, 160)  480         conv2d_801[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_806 (BatchN (None, 29, 29, 160)  480         conv2d_806[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_800 (Activation)     (None, 29, 29, 160)  0           batch_normalization_801[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_805 (Activation)     (None, 29, 29, 160)  0           batch_normalization_806[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_802 (Conv2D)             (None, 29, 29, 160)  179200      activation_800[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_807 (Conv2D)             (None, 29, 29, 160)  179200      activation_805[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_802 (BatchN (None, 29, 29, 160)  480         conv2d_802[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_807 (BatchN (None, 29, 29, 160)  480         conv2d_807[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_801 (Activation)     (None, 29, 29, 160)  0           batch_normalization_802[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_806 (Activation)     (None, 29, 29, 160)  0           batch_normalization_807[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_77 (AveragePo (None, 29, 29, 768)  0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_800 (Conv2D)             (None, 29, 29, 192)  147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_803 (Conv2D)             (None, 29, 29, 192)  215040      activation_801[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_808 (Conv2D)             (None, 29, 29, 192)  215040      activation_806[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_809 (Conv2D)             (None, 29, 29, 192)  147456      average_pooling2d_77[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_800 (BatchN (None, 29, 29, 192)  576         conv2d_800[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_803 (BatchN (None, 29, 29, 192)  576         conv2d_803[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_808 (BatchN (None, 29, 29, 192)  576         conv2d_808[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_809 (BatchN (None, 29, 29, 192)  576         conv2d_809[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_799 (Activation)     (None, 29, 29, 192)  0           batch_normalization_800[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_802 (Activation)     (None, 29, 29, 192)  0           batch_normalization_803[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_807 (Activation)     (None, 29, 29, 192)  0           batch_normalization_808[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_808 (Activation)     (None, 29, 29, 192)  0           batch_normalization_809[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 29, 29, 768)  0           activation_799[0][0]             \n",
      "                                                                 activation_802[0][0]             \n",
      "                                                                 activation_807[0][0]             \n",
      "                                                                 activation_808[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_814 (Conv2D)             (None, 29, 29, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_814 (BatchN (None, 29, 29, 192)  576         conv2d_814[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_813 (Activation)     (None, 29, 29, 192)  0           batch_normalization_814[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_815 (Conv2D)             (None, 29, 29, 192)  258048      activation_813[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_815 (BatchN (None, 29, 29, 192)  576         conv2d_815[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_814 (Activation)     (None, 29, 29, 192)  0           batch_normalization_815[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_811 (Conv2D)             (None, 29, 29, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_816 (Conv2D)             (None, 29, 29, 192)  258048      activation_814[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_811 (BatchN (None, 29, 29, 192)  576         conv2d_811[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_816 (BatchN (None, 29, 29, 192)  576         conv2d_816[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_810 (Activation)     (None, 29, 29, 192)  0           batch_normalization_811[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_815 (Activation)     (None, 29, 29, 192)  0           batch_normalization_816[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_812 (Conv2D)             (None, 29, 29, 192)  258048      activation_810[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_817 (Conv2D)             (None, 29, 29, 192)  258048      activation_815[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_812 (BatchN (None, 29, 29, 192)  576         conv2d_812[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_817 (BatchN (None, 29, 29, 192)  576         conv2d_817[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_811 (Activation)     (None, 29, 29, 192)  0           batch_normalization_812[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_816 (Activation)     (None, 29, 29, 192)  0           batch_normalization_817[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_78 (AveragePo (None, 29, 29, 768)  0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_810 (Conv2D)             (None, 29, 29, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_813 (Conv2D)             (None, 29, 29, 192)  258048      activation_811[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_818 (Conv2D)             (None, 29, 29, 192)  258048      activation_816[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_819 (Conv2D)             (None, 29, 29, 192)  147456      average_pooling2d_78[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_810 (BatchN (None, 29, 29, 192)  576         conv2d_810[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_813 (BatchN (None, 29, 29, 192)  576         conv2d_813[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_818 (BatchN (None, 29, 29, 192)  576         conv2d_818[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_819 (BatchN (None, 29, 29, 192)  576         conv2d_819[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_809 (Activation)     (None, 29, 29, 192)  0           batch_normalization_810[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_812 (Activation)     (None, 29, 29, 192)  0           batch_normalization_813[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_817 (Activation)     (None, 29, 29, 192)  0           batch_normalization_818[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_818 (Activation)     (None, 29, 29, 192)  0           batch_normalization_819[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 29, 29, 768)  0           activation_809[0][0]             \n",
      "                                                                 activation_812[0][0]             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 activation_817[0][0]             \n",
      "                                                                 activation_818[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_822 (Conv2D)             (None, 29, 29, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_822 (BatchN (None, 29, 29, 192)  576         conv2d_822[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_821 (Activation)     (None, 29, 29, 192)  0           batch_normalization_822[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_823 (Conv2D)             (None, 29, 29, 192)  258048      activation_821[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_823 (BatchN (None, 29, 29, 192)  576         conv2d_823[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_822 (Activation)     (None, 29, 29, 192)  0           batch_normalization_823[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_820 (Conv2D)             (None, 29, 29, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_824 (Conv2D)             (None, 29, 29, 192)  258048      activation_822[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_820 (BatchN (None, 29, 29, 192)  576         conv2d_820[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_824 (BatchN (None, 29, 29, 192)  576         conv2d_824[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_819 (Activation)     (None, 29, 29, 192)  0           batch_normalization_820[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_823 (Activation)     (None, 29, 29, 192)  0           batch_normalization_824[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_821 (Conv2D)             (None, 14, 14, 320)  552960      activation_819[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_825 (Conv2D)             (None, 14, 14, 192)  331776      activation_823[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_821 (BatchN (None, 14, 14, 320)  960         conv2d_821[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_825 (BatchN (None, 14, 14, 192)  576         conv2d_825[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_820 (Activation)     (None, 14, 14, 320)  0           batch_normalization_821[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_824 (Activation)     (None, 14, 14, 192)  0           batch_normalization_825[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling2D) (None, 14, 14, 768)  0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 14, 14, 1280) 0           activation_820[0][0]             \n",
      "                                                                 activation_824[0][0]             \n",
      "                                                                 max_pooling2d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_830 (Conv2D)             (None, 14, 14, 448)  573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_830 (BatchN (None, 14, 14, 448)  1344        conv2d_830[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_829 (Activation)     (None, 14, 14, 448)  0           batch_normalization_830[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_827 (Conv2D)             (None, 14, 14, 384)  491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_831 (Conv2D)             (None, 14, 14, 384)  1548288     activation_829[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_827 (BatchN (None, 14, 14, 384)  1152        conv2d_827[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_831 (BatchN (None, 14, 14, 384)  1152        conv2d_831[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_826 (Activation)     (None, 14, 14, 384)  0           batch_normalization_827[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_830 (Activation)     (None, 14, 14, 384)  0           batch_normalization_831[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_828 (Conv2D)             (None, 14, 14, 384)  442368      activation_826[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_829 (Conv2D)             (None, 14, 14, 384)  442368      activation_826[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_832 (Conv2D)             (None, 14, 14, 384)  442368      activation_830[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_833 (Conv2D)             (None, 14, 14, 384)  442368      activation_830[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_79 (AveragePo (None, 14, 14, 1280) 0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_826 (Conv2D)             (None, 14, 14, 320)  409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_828 (BatchN (None, 14, 14, 384)  1152        conv2d_828[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_829 (BatchN (None, 14, 14, 384)  1152        conv2d_829[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_832 (BatchN (None, 14, 14, 384)  1152        conv2d_832[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_833 (BatchN (None, 14, 14, 384)  1152        conv2d_833[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_834 (Conv2D)             (None, 14, 14, 192)  245760      average_pooling2d_79[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_826 (BatchN (None, 14, 14, 320)  960         conv2d_826[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_827 (Activation)     (None, 14, 14, 384)  0           batch_normalization_828[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_828 (Activation)     (None, 14, 14, 384)  0           batch_normalization_829[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_831 (Activation)     (None, 14, 14, 384)  0           batch_normalization_832[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_832 (Activation)     (None, 14, 14, 384)  0           batch_normalization_833[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_834 (BatchN (None, 14, 14, 192)  576         conv2d_834[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_825 (Activation)     (None, 14, 14, 320)  0           batch_normalization_826[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 14, 14, 768)  0           activation_827[0][0]             \n",
      "                                                                 activation_828[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 14, 14, 768)  0           activation_831[0][0]             \n",
      "                                                                 activation_832[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_833 (Activation)     (None, 14, 14, 192)  0           batch_normalization_834[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 14, 14, 2048) 0           activation_825[0][0]             \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_16[0][0]             \n",
      "                                                                 activation_833[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_839 (Conv2D)             (None, 14, 14, 448)  917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_839 (BatchN (None, 14, 14, 448)  1344        conv2d_839[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_838 (Activation)     (None, 14, 14, 448)  0           batch_normalization_839[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_836 (Conv2D)             (None, 14, 14, 384)  786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_840 (Conv2D)             (None, 14, 14, 384)  1548288     activation_838[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_836 (BatchN (None, 14, 14, 384)  1152        conv2d_836[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_840 (BatchN (None, 14, 14, 384)  1152        conv2d_840[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_835 (Activation)     (None, 14, 14, 384)  0           batch_normalization_836[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_839 (Activation)     (None, 14, 14, 384)  0           batch_normalization_840[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_837 (Conv2D)             (None, 14, 14, 384)  442368      activation_835[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_838 (Conv2D)             (None, 14, 14, 384)  442368      activation_835[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_841 (Conv2D)             (None, 14, 14, 384)  442368      activation_839[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_842 (Conv2D)             (None, 14, 14, 384)  442368      activation_839[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_80 (AveragePo (None, 14, 14, 2048) 0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_835 (Conv2D)             (None, 14, 14, 320)  655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_837 (BatchN (None, 14, 14, 384)  1152        conv2d_837[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_838 (BatchN (None, 14, 14, 384)  1152        conv2d_838[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_841 (BatchN (None, 14, 14, 384)  1152        conv2d_841[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_842 (BatchN (None, 14, 14, 384)  1152        conv2d_842[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_843 (Conv2D)             (None, 14, 14, 192)  393216      average_pooling2d_80[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_835 (BatchN (None, 14, 14, 320)  960         conv2d_835[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_836 (Activation)     (None, 14, 14, 384)  0           batch_normalization_837[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_837 (Activation)     (None, 14, 14, 384)  0           batch_normalization_838[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_840 (Activation)     (None, 14, 14, 384)  0           batch_normalization_841[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_841 (Activation)     (None, 14, 14, 384)  0           batch_normalization_842[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_843 (BatchN (None, 14, 14, 192)  576         conv2d_843[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_834 (Activation)     (None, 14, 14, 320)  0           batch_normalization_835[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 14, 14, 768)  0           activation_836[0][0]             \n",
      "                                                                 activation_837[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 14, 14, 768)  0           activation_840[0][0]             \n",
      "                                                                 activation_841[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_842 (Activation)     (None, 14, 14, 192)  0           batch_normalization_843[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 14, 14, 2048) 0           activation_834[0][0]             \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_17[0][0]             \n",
      "                                                                 activation_842[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_8 (Glo (None, 2048)         0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 2048)         0           global_average_pooling2d_8[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 2)            4098        dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 21,806,882\n",
      "Trainable params: 21,772,450\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Top Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 45s 1us/step\n",
      "Epoch 1/99\n",
      " 6/46 [==>...........................] - ETA: 34:20 - loss: 0.6568 - acc: 0.6250"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9d28045468cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                     callbacks=callback_list)                    \n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Top Model Train Done.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    190\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1218\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ** Configuation **\n",
    "# Freeze layers (Only top trainable)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "loss = losses.categorical_crossentropy\n",
    "optimizer = optimizers.Adam()\n",
    "metrics = ['accuracy']\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "# Callbacks\n",
    "m_q = 'val_loss'\n",
    "model_path = './models/model_foodvsnot_v2.h5'\n",
    "check_pt = callbacks.ModelCheckpoint(filepath=model_path, monitor=m_q, save_best_only=True, verbose=1)\n",
    "early_stop = callbacks.EarlyStopping(patience=1, monitor=m_q, verbose=1)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(patience=0, factor=0.33, monitor=m_q, verbose=1)\n",
    "callback_list = [check_pt, early_stop, reduce_lr]\n",
    "\n",
    "\n",
    "# ** Fit **\n",
    "# Data Generator\n",
    "datagen = ImageDataGenerator(horizontal_flip=True,  \n",
    "                             rotation_range=15,\n",
    "                             fill_mode='reflect',                          \n",
    "                             preprocessing_function=preprocess_im)\n",
    "\n",
    "\n",
    "# Batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Fit\n",
    "model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "                    validation_data=(X_val, y_val),  \n",
    "                    epochs=99,\n",
    "                    steps_per_epoch=len(X_train)/batch_size,\n",
    "                    callbacks=callback_list)                    \n",
    "print('Top Model Train Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train All Model - Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = load_model('./models/model_foodvsnot_v2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/99\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9760Epoch 00000: val_loss improved from inf to 0.05884, saving model to ./models/model_foodvsnot_v2.h5\n",
      "46/46 [==============================] - 51s - loss: 0.0754 - acc: 0.9755 - val_loss: 0.0588 - val_acc: 0.9779\n",
      "Epoch 2/99\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9888Epoch 00001: val_loss improved from 0.05884 to 0.05426, saving model to ./models/model_foodvsnot_v2.h5\n",
      "46/46 [==============================] - 39s - loss: 0.0356 - acc: 0.9890 - val_loss: 0.0543 - val_acc: 0.9809\n",
      "Epoch 3/99\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9924Epoch 00002: val_loss improved from 0.05426 to 0.05081, saving model to ./models/model_foodvsnot_v2.h5\n",
      "46/46 [==============================] - 39s - loss: 0.0265 - acc: 0.9925 - val_loss: 0.0508 - val_acc: 0.9799\n",
      "Epoch 4/99\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9954Epoch 00003: val_loss improved from 0.05081 to 0.04130, saving model to ./models/model_foodvsnot_v2.h5\n",
      "46/46 [==============================] - 39s - loss: 0.0218 - acc: 0.9951 - val_loss: 0.0413 - val_acc: 0.9840\n",
      "Epoch 5/99\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9958Epoch 00004: val_loss improved from 0.04130 to 0.03981, saving model to ./models/model_foodvsnot_v2.h5\n",
      "46/46 [==============================] - 39s - loss: 0.0138 - acc: 0.9959 - val_loss: 0.0398 - val_acc: 0.9829\n",
      "Epoch 6/99\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9957Epoch 00005: val_loss improved from 0.03981 to 0.03668, saving model to ./models/model_foodvsnot_v2.h5\n",
      "46/46 [==============================] - 39s - loss: 0.0143 - acc: 0.9958 - val_loss: 0.0367 - val_acc: 0.9840\n",
      "Epoch 7/99\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9979Epoch 00006: val_loss did not improve\n",
      "\n",
      "Epoch 00006: reducing learning rate to 0.000330000015674.\n",
      "46/46 [==============================] - 39s - loss: 0.0094 - acc: 0.9980 - val_loss: 0.0373 - val_acc: 0.9829\n",
      "Epoch 8/99\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9969Epoch 00007: val_loss improved from 0.03668 to 0.03558, saving model to ./models/model_foodvsnot_v2.h5\n",
      "46/46 [==============================] - 38s - loss: 0.0099 - acc: 0.9969 - val_loss: 0.0356 - val_acc: 0.9840\n",
      "Epoch 9/99\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9990Epoch 00008: val_loss improved from 0.03558 to 0.03498, saving model to ./models/model_foodvsnot_v2.h5\n",
      "46/46 [==============================] - 39s - loss: 0.0062 - acc: 0.9990 - val_loss: 0.0350 - val_acc: 0.9840\n",
      "Epoch 10/99\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9979Epoch 00009: val_loss improved from 0.03498 to 0.03403, saving model to ./models/model_foodvsnot_v2.h5\n",
      "46/46 [==============================] - 39s - loss: 0.0079 - acc: 0.9980 - val_loss: 0.0340 - val_acc: 0.9850\n",
      "Epoch 11/99\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9997Epoch 00010: val_loss improved from 0.03403 to 0.03334, saving model to ./models/model_foodvsnot_v2.h5\n",
      "46/46 [==============================] - 39s - loss: 0.0057 - acc: 0.9997 - val_loss: 0.0333 - val_acc: 0.9850\n",
      "Epoch 12/99\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9988Epoch 00011: val_loss did not improve\n",
      "\n",
      "Epoch 00011: reducing learning rate to 0.000108900003252.\n",
      "46/46 [==============================] - 38s - loss: 0.0055 - acc: 0.9989 - val_loss: 0.0334 - val_acc: 0.9860\n",
      "Epoch 13/99\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9986Epoch 00012: val_loss improved from 0.03334 to 0.03306, saving model to ./models/model_foodvsnot_v2.h5\n",
      "46/46 [==============================] - 39s - loss: 0.0068 - acc: 0.9986 - val_loss: 0.0331 - val_acc: 0.9860\n",
      "Epoch 14/99\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9993Epoch 00013: val_loss did not improve\n",
      "\n",
      "Epoch 00013: reducing learning rate to 3.59369999205e-05.\n",
      "46/46 [==============================] - 38s - loss: 0.0048 - acc: 0.9993 - val_loss: 0.0333 - val_acc: 0.9860\n",
      "Epoch 15/99\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9957Epoch 00014: val_loss did not improve\n",
      "\n",
      "Epoch 00014: reducing learning rate to 1.18592095896e-05.\n",
      "46/46 [==============================] - 38s - loss: 0.0114 - acc: 0.9958 - val_loss: 0.0332 - val_acc: 0.9860\n",
      "Epoch 00014: early stopping\n",
      "All Model Train Done.\n"
     ]
    }
   ],
   "source": [
    "# ** Configuation **\n",
    "# Open layers\n",
    "# for layer in model.layers[:249]:\n",
    "#     layer.trainable = False\n",
    "# for layer in model.layers[249:]:\n",
    "#     layer.trainable = True\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Compile the model\n",
    "loss = losses.categorical_crossentropy\n",
    "optimizer = optimizers.SGD(lr=0.001, momentum=0.9)\n",
    "metrics = ['accuracy']\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "# Callbacks\n",
    "m_q = 'val_loss'\n",
    "model_path = './models/model_foodvsnot_v2.h5'\n",
    "check_pt = callbacks.ModelCheckpoint(filepath=model_path, monitor=m_q, save_best_only=True, verbose=1)\n",
    "early_stop = callbacks.EarlyStopping(patience=1, monitor=m_q, verbose=1)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(patience=0, factor=0.33, monitor=m_q, verbose=1)\n",
    "callback_list = [check_pt, early_stop, reduce_lr]\n",
    "\n",
    "\n",
    "# ** Fit **\n",
    "# Data Generator\n",
    "datagen = ImageDataGenerator(horizontal_flip=True,  \n",
    "                             rotation_range=15,\n",
    "                             fill_mode='reflect',                          \n",
    "                             preprocessing_function=preprocess_im)\n",
    "\n",
    "\n",
    "# Batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Fit\n",
    "model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "                    validation_data=(X_val, y_val),  \n",
    "                    epochs=99,\n",
    "                    steps_per_epoch=len(X_train)/batch_size,\n",
    "                    callbacks=callback_list)                    \n",
    "print 'All Model Train Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Find Optimal Threshold\n",
    "\n",
    "Find optimal threshold by using validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = load_model('./models/model_foodvsnot.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "997/997 [==============================] - 5s     \n"
     ]
    }
   ],
   "source": [
    "# Predict validation data\n",
    "p_val = model.predict(inception_v3.preprocess_input(X_val.astype(np.float32)), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimum Threshold: 0.766766766767\n",
      "Max. Accuracy: 0.98294884654\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmQAAAF3CAYAAAALu1cUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5349267d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate thresholds and accuracies\n",
    "thresholds = np.linspace(0, 1, 1000)\n",
    "accuracies = [np.mean(y_val == (p_val[:, 1] > t).astype(np.int)) for t in thresholds]\n",
    "# f1_scores = [metrics.f1_score(y_val, (p_val[:, 1] > t)) for t in thresholds]\n",
    "# Find optimum threshold\n",
    "optimum_pair = sorted(zip(thresholds, accuracies), key=lambda x: x[1])[-1]\n",
    "print 'Optimum Threshold:', optimum_pair[0]\n",
    "print 'Max. Accuracy:', optimum_pair[1]\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, accuracies)\n",
    "# plt.plot(thresholds, f1_scores)\n",
    "plt.xlabel('Threshold');\n",
    "plt.ylabel('Score');\n",
    "plt.grid('on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
