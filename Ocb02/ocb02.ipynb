{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'preprocessing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4f6f41d6aac0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# from networks.my_inception_v3 import myInceptionV3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_annotation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named 'preprocessing'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from keras.layers import Reshape, Conv2D, Input, Lambda, UpSampling2D, MaxPooling2D, LeakyReLU, BatchNormalization\n",
    "from keras import layers\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.merge import concatenate\n",
    "from keras_applications.mobilenet import _depthwise_conv_block\n",
    "from keras_applications.mobilenet_v2 import _inverted_res_block\n",
    "\n",
    "# from networks.my_inception_v3 import myInceptionV3\n",
    "from preprocessing import parse_annotation, BatchGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0',\n",
       " '/job:localhost/replica:0/task:0/device:GPU:1']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check gpu resources\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gpu to do training\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_category():\n",
    "    category = []\n",
    "    with open('./UECFOOD100_JS/category.txt', 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            if i > 0:\n",
    "                line = line.rstrip('\\n')\n",
    "                line = line.split('\\t')\n",
    "                category.append(line[1])\n",
    "    return category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parse_annotation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0fe88312e363>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mannot_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./UECFOOD100_JS/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/annotations_new/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mfolder_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseen_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_annotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannot_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mall_imgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parse_annotation' is not defined"
     ]
    }
   ],
   "source": [
    "''' Initiailize parameters '''\n",
    "LABELS = read_category()\n",
    "\n",
    "IMAGE_H, IMAGE_W = 224, 224  # must equal to GRID_H * 32  416, 416\n",
    "GRID_H, GRID_W = 7, 7        # 13, 13\n",
    "N_BOX = 5\n",
    "CLASS = len(LABELS)\n",
    "CLASS_WEIGHTS = np.ones(CLASS, dtype='float32')\n",
    "OBJ_THRESHOLD = 0.3\n",
    "NMS_THRESHOLD = 0.3\n",
    "\n",
    "# Read knn generated anchor_5.txt\n",
    "ANCHORS = []\n",
    "with open('./UECFOOD100_JS/generated_anchors_mobilenet/anchors_5.txt', 'r') as anchor_file:\n",
    "    for i, line in enumerate(anchor_file):\n",
    "        line = line.rstrip('\\n')\n",
    "        ANCHORS.append(list(map(float, line.split(', '))))\n",
    "ANCHORS = list(list(np.array(ANCHORS).reshape(1, -1))[0])\n",
    "\n",
    "NO_OBJECT_SCALE = 1.0\n",
    "OBJECT_SCALE = 5.0\n",
    "COORD_SCALE = 1.0\n",
    "CLASS_SCALE = 1.0\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "WARM_UP_BATCHES = 100\n",
    "TRUE_BOX_BUFFER = 15\n",
    "\n",
    "generator_config = {\n",
    "    'IMAGE_H': IMAGE_H,\n",
    "    'IMAGE_W': IMAGE_W,\n",
    "    'GRID_H': GRID_H,\n",
    "    'GRID_W': GRID_W,\n",
    "    'BOX': N_BOX,\n",
    "    'LABELS': LABELS,\n",
    "    'CLASS': len(LABELS),\n",
    "    'ANCHORS': ANCHORS,\n",
    "    'BATCH_SIZE': BATCH_SIZE,\n",
    "    'TRUE_BOX_BUFFER': TRUE_BOX_BUFFER,\n",
    "}\n",
    "\n",
    "all_imgs = []\n",
    "for i in range(0, len(LABELS)):\n",
    "    image_path = './UECFOOD100_JS/' + str(i+1) + '/'\n",
    "    annot_path = './UECFOOD100_JS/' + str(i+1) + '/' + '/annotations_new/'\n",
    "\n",
    "    folder_imgs, seen_labels = parse_annotation(annot_path, image_path)\n",
    "    all_imgs.extend(folder_imgs)\n",
    "print(np.array(all_imgs).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def space_to_depth_x2(x):\n",
    "    return tf.space_to_depth(x, block_size=2)\n",
    "\n",
    "def normalize(image):\n",
    "    return image / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrained_mn1():\n",
    "    alpha, depth_multiplier = 1, 1\n",
    "    print('=> Building new model with pretrained MobilenetV1...')\n",
    "\n",
    "    pretrained_gap_model = load_model('./models/gap_foodnotfood_mn1_224.h5')\n",
    "    print(pretrained_gap_model.summary())\n",
    "    model = Model(inputs=pretrained_gap_model.input, outputs=pretrained_gap_model.layers[-6].input)\n",
    "    print(model.summary())\n",
    "    x = model(input_image)\n",
    "    x = _depthwise_conv_block(x, 1024, alpha, depth_multiplier,\n",
    "                              strides=(2, 2), block_id=12)\n",
    "    x = _depthwise_conv_block(x, 1024, alpha, depth_multiplier, block_id=13)\n",
    "\n",
    "    x = Conv2D(N_BOX * (4 + 1 + CLASS), (1, 1), strides=(1, 1), padding='same', name='conv_23')(x)\n",
    "    output = Reshape((GRID_H, GRID_W, N_BOX, 4 + 1 + CLASS))(x)\n",
    "    output = Lambda(lambda args: args[0])([output, true_boxes])\n",
    "\n",
    "    model = Model([input_image, true_boxes], output)\n",
    "    print(model.summary())\n",
    "    print('Finish new model.')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "\n",
    "    layer = model.layers[-4]            # the last convolutional layer\n",
    "    weights = layer.get_weights()\n",
    "\n",
    "    new_kernel = np.random.normal(size=weights[0].shape) / (GRID_H * GRID_W)\n",
    "    new_bias = np.random.normal(size=weights[1].shape) / (GRID_H * GRID_W)\n",
    "\n",
    "    layer.set_weights([new_kernel, new_bias])\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss',\n",
    "                               min_delta=0.001,\n",
    "                               patience=3,\n",
    "                               mode='min',\n",
    "                               verbose=1)\n",
    "\n",
    "    checkpoint = ModelCheckpoint('dlp_tla_mn224_e1_1001.h5',\n",
    "                                 monitor='val_loss',\n",
    "                                 verbose=1,\n",
    "                                 save_best_only=True,\n",
    "                                 mode='min',\n",
    "                                 period=1)\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(patience=0, factor=0.2, monitor='val_loss', verbose=1)\n",
    "\n",
    "    # model.load_weights('./transferLearning_mn_224_0_04425.h5')\n",
    "\n",
    "    tb_counter = len([log for log in os.listdir(os.path.expanduser('./tl_tf_logs/')) if 'uecfood100' in log]) + 1\n",
    "    tensorboard = TensorBoard(log_dir=os.path.expanduser('~/tf_log/') + 'transferLearning_uecfood100' + '_' + str(tb_counter),\n",
    "                              histogram_freq=0,\n",
    "                              write_graph=True,\n",
    "                              write_images=False)\n",
    "\n",
    "    # TODO: try different optimizer and tweak parameters (in MNv1 paper they used RMSprop)\n",
    "    optimizer = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    # optimizer = SGD(lr=1e-4, decay=0.0005, momentum=0.9)\n",
    "    # optimizer = RMSprop(lr=1e-5, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    model.compile(loss=custom_loss, optimizer=optimizer)\n",
    "\n",
    "    hist = model.fit_generator(generator=train_batch,\n",
    "                               steps_per_epoch=len(train_batch),\n",
    "                               epochs=20,      # 100\n",
    "                               verbose=1,\n",
    "                               validation_data=valid_batch,\n",
    "                               validation_steps=len(valid_batch),\n",
    "                               callbacks=[early_stop, checkpoint, tensorboard, reduce_lr],\n",
    "                               max_queue_size=3)\n",
    "\n",
    "    loss_hist = hist.history['loss']\n",
    "    np_loss_hist = np.array(loss_hist)\n",
    "    np.savetxt('loss_history_dlp_tla_mn224_e20_1001.txt', np_loss_hist, delimiter=',')\n",
    "\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    mask_shape = tf.shape(y_true)[:4]\n",
    "\n",
    "    cell_x = tf.to_float(tf.reshape(tf.tile(tf.range(GRID_W), [GRID_H]), (1, GRID_H, GRID_W, 1, 1)))\n",
    "    cell_y = tf.transpose(cell_x, (0, 2, 1, 3, 4))\n",
    "\n",
    "    cell_grid = tf.tile(tf.concat([cell_x, cell_y], -1), [BATCH_SIZE, 1, 1, 5, 1])\n",
    "\n",
    "    coord_mask = tf.zeros(mask_shape)\n",
    "    conf_mask = tf.zeros(mask_shape)\n",
    "    class_mask = tf.zeros(mask_shape)\n",
    "\n",
    "    seen = tf.Variable(0.)\n",
    "    total_recall = tf.Variable(0.)\n",
    "\n",
    "    \"\"\" Adjust prediction \"\"\"\n",
    "    # adjust x and y\n",
    "    pred_box_xy = tf.sigmoid(y_pred[..., :2]) + cell_grid\n",
    "\n",
    "    # adjust w and h\n",
    "    pred_box_wh = tf.exp(y_pred[..., 2:4]) * np.reshape(ANCHORS, [1, 1, 1, N_BOX, 2])\n",
    "\n",
    "    # adjust confidence\n",
    "    pred_box_conf = tf.sigmoid(y_pred[..., 4])\n",
    "\n",
    "    # adjust class probabilities\n",
    "    pred_box_class = y_pred[..., 5:]\n",
    "\n",
    "    \"\"\" Adjust ground truth \"\"\"\n",
    "    # adjust x and y\n",
    "    true_box_xy = y_true[..., 0:2]  # relative position to the containing cell\n",
    "\n",
    "    # adjust w and h\n",
    "    true_box_wh = y_true[..., 2:4]  # number of cells accross, horizontally and vertically\n",
    "\n",
    "    # adjust confidence\n",
    "    true_wh_half = true_box_wh / 2.\n",
    "    true_mins = true_box_xy - true_wh_half\n",
    "    true_maxes = true_box_xy + true_wh_half\n",
    "\n",
    "    pred_wh_half = pred_box_wh / 2.\n",
    "    pred_mins = pred_box_xy - pred_wh_half\n",
    "    pred_maxes = pred_box_xy + pred_wh_half\n",
    "\n",
    "    intersect_mins = tf.maximum(pred_mins, true_mins)\n",
    "    intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
    "    intersect_wh = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "\n",
    "    true_areas = true_box_wh[..., 0] * true_box_wh[..., 1]\n",
    "    pred_areas = pred_box_wh[..., 0] * pred_box_wh[..., 1]\n",
    "\n",
    "    union_areas = pred_areas + true_areas - intersect_areas\n",
    "    iou_scores = tf.truediv(intersect_areas, union_areas)\n",
    "\n",
    "    true_box_conf = iou_scores * y_true[..., 4]\n",
    "\n",
    "    # adjust class probabilities\n",
    "    true_box_class = tf.argmax(y_true[..., 5:], -1)\n",
    "\n",
    "    \"\"\" Determine the masks \"\"\"\n",
    "    # coordinate mask: simply the position of the ground truth boxes (the predictors)\n",
    "    coord_mask = tf.expand_dims(y_true[..., 4], axis=-1) * COORD_SCALE\n",
    "\n",
    "    # confidence mask: penelize predictors + penalize boxes with low IOU\n",
    "    # penalize the confidence of the boxes, which have IOU with some ground truth box < 0.6\n",
    "    true_xy = true_boxes[..., 0:2]\n",
    "    true_wh = true_boxes[..., 2:4]\n",
    "\n",
    "    true_wh_half = true_wh / 2.\n",
    "    true_mins = true_xy - true_wh_half\n",
    "    true_maxes = true_xy + true_wh_half\n",
    "\n",
    "    pred_xy = tf.expand_dims(pred_box_xy, 4)\n",
    "    pred_wh = tf.expand_dims(pred_box_wh, 4)\n",
    "\n",
    "    pred_wh_half = pred_wh / 2.\n",
    "    pred_mins = pred_xy - pred_wh_half\n",
    "    pred_maxes = pred_xy + pred_wh_half\n",
    "\n",
    "    intersect_mins = tf.maximum(pred_mins, true_mins)\n",
    "    intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
    "    intersect_wh = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "\n",
    "    true_areas = true_wh[..., 0] * true_wh[..., 1]\n",
    "    pred_areas = pred_wh[..., 0] * pred_wh[..., 1]\n",
    "\n",
    "    union_areas = pred_areas + true_areas - intersect_areas\n",
    "    iou_scores = tf.truediv(intersect_areas, union_areas)\n",
    "\n",
    "    best_ious = tf.reduce_max(iou_scores, axis=4)\n",
    "    conf_mask = conf_mask + tf.to_float(best_ious < 0.6) * (1 - y_true[..., 4]) * NO_OBJECT_SCALE\n",
    "\n",
    "    # penalize the confidence of the boxes, which are reponsible for corresponding ground truth box\n",
    "    conf_mask = conf_mask + y_true[..., 4] * OBJECT_SCALE\n",
    "\n",
    "    # class mask: simply the position of the ground truth boxes (the predictors)\n",
    "    class_mask = y_true[..., 4] * tf.gather(CLASS_WEIGHTS, true_box_class) * CLASS_SCALE\n",
    "\n",
    "    \"\"\" Warm-up training \"\"\"\n",
    "    no_boxes_mask = tf.to_float(coord_mask < COORD_SCALE / 2.)\n",
    "    seen = tf.assign_add(seen, 1.)\n",
    "\n",
    "    true_box_xy, true_box_wh, coord_mask = tf.cond(tf.less(seen, WARM_UP_BATCHES),\n",
    "                                                   lambda: [true_box_xy + (0.5 + cell_grid) * no_boxes_mask,\n",
    "                                                            true_box_wh + tf.ones_like(true_box_wh) * np.reshape(\n",
    "                                                                ANCHORS, [1, 1, 1, N_BOX, 2]) * no_boxes_mask,\n",
    "                                                            tf.ones_like(coord_mask)],\n",
    "                                                   lambda: [true_box_xy,\n",
    "                                                            true_box_wh,\n",
    "                                                            coord_mask])\n",
    "\n",
    "    \"\"\" Finalize the loss \"\"\"\n",
    "    nb_coord_box = tf.reduce_sum(tf.to_float(coord_mask > 0.0))\n",
    "    nb_conf_box = tf.reduce_sum(tf.to_float(conf_mask > 0.0))\n",
    "    nb_class_box = tf.reduce_sum(tf.to_float(class_mask > 0.0))\n",
    "\n",
    "    loss_xy = tf.reduce_sum(tf.square(true_box_xy - pred_box_xy) * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
    "    loss_wh = tf.reduce_sum(tf.square(true_box_wh - pred_box_wh) * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
    "    loss_conf = tf.reduce_sum(tf.square(true_box_conf - pred_box_conf) * conf_mask) / (nb_conf_box + 1e-6) / 2.\n",
    "    loss_class = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=true_box_class, logits=pred_box_class)\n",
    "    loss_class = tf.reduce_sum(loss_class * class_mask) / (nb_class_box + 1e-6)\n",
    "\n",
    "    loss = loss_xy + loss_wh + loss_conf + loss_class\n",
    "\n",
    "    nb_true_box = tf.reduce_sum(y_true[..., 4])\n",
    "    nb_pred_box = tf.reduce_sum(tf.to_float(true_box_conf > 0.5) * tf.to_float(pred_box_conf > 0.3))\n",
    "\n",
    "    \"\"\" Debugging code \"\"\"\n",
    "    current_recall = nb_pred_box / (nb_true_box + 1e-6)\n",
    "    total_recall = tf.assign_add(total_recall, current_recall)\n",
    "\n",
    "    loss = tf.Print(loss, [tf.zeros((1))], message='\\nDummy Line \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_xy], message='Loss XY \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_wh], message='Loss WH \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_conf], message='Loss Conf \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_class], message='Loss Class \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss], message='Total Loss \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [current_recall], message='Current Recall \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [total_recall / seen], message='Average Recall \\t', summarize=1000)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
